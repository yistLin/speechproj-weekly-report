<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <title>Weekly Report III</title>
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white_edit.css" id="theme">
    <link rel="stylesheet" href="css/prism.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">

        <section>
          <h1>Weekly Report IV</h1>
          <p>林義聖、陳力宇</p>
          <p>December 1, 2016</p>
        </section>

        <!-- Week 9 -->
        <section>
          <h2>聊天機器人</h2>
          <ul>
            <li>Response meaningful sentence</li>
            <li>Understand natural language</li>
          </ul>
        </section>

        <section>
          <section>
            <h2>Attention</h2>
          </section>
          <section>
            <h3>Attention</h3>
            <ul>
              <li>A recent trend in Deep Learning are Attention Mechanisms</li>
              <li>They are loosely based on the visual attention mechanism found in humans</li>
              <li>Being able to focus on a certain region of an image with “high resolution” while perceiving the surrounding image in “low resolution”</li>
            </ul>
            <p><small>Reference: <u>http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/</u></small></p>
          </section>
          <section>
            <figure>
              <img src="images/focal.jpg" alt="focal.jpg" width="70%">
              <figcaption><small>photoed by Yist Lin</small></figcaption>
            </figure>
          </section>
          <section>
            <h3>Original Neural Machine Translation</h3>
            <ul>
              <li>Encode a sentence into a fixed-dimensional vector</li>
              <li>Use LSTM to handle long term dependencies</li>
              <li>Reverse source sentences or feeding them twice to help a network to better memorize things</li>
            </ul>
          </section>
          <section>
            <h3>Neural Machine Translation with Attention</h3>
            <ul>
              <li>No need to encode source sentences</li>
              <li>Each decoder output word now depends on a weighted combination of all the input states, not just the last state</li>
              <li><i>Disadvantages:</i> We need to calculate an attention value for each combination of input and output word</li>
            </ul>
          </section>
        </section>

        <section>
          <section>
            <h2>Bucketing &amp; Padding</h2>
          </section>
          <section>
            <h3>Why bucketing?</h3>
            <ul>
              <li>RNN can handle sentences with different lengths.</li>
              <li>Tensor must be a fixed size.</li>
              <li>Create a seq2seq model for every pair of lengths?</li>
              <ul>
                <li>This would result in an enormous graph consisting of many very similar subgraphs.</li>
              </ul>
            </ul>
          </section>
          <section>
            <h3>What is bucketing?</h3>
            <pre><code class="language-python" data-trim>
buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]
            </code></pre>
            <ul>
              <li>Use the minimum bucket but still larger than the length of input or output sentence.</li>
              <li>Fixed size?</li>
            </ul>
          </section>
          <section>
            <h3>Padding</h3>
            <p>Pad every sentence with a special PAD symbol.</p>
            <pre><code class="language">
  input    : ["I", "go", "."]       →  [PAD PAD "." "go" "I"]
  output : ["Je", "vais", "."]  →  [GO "Je" "vais" "." EOS PAD PAD PAD PAD PAD]
            </code></pre>
            <figcaption><small>Use the (5, 10) bucket.</small></figcaption>
          </section>
        </section>

        <section>
          <section>
            <h2>Neural Conversation Models</h2>
          </section>
        </section>

        <section>
          <section>
            <h2>Experiment</h2>
          </section>
          <section>
            <h3>Experiment</h3>
            <ul>
              <li><i>Training Data:</i> TSV format Ubuntu Dialog Data</li>
              <li><i>Model:</i> Attention based seq2seq model</li>
            </ul>
            <p><small>Reference: <u>https://github.com/pbhatia243/Neural_Conversation_Models</u></small></p>
          </section>
          <section>
            <h3>Environment &amp; Result</h3>
            <ul>
              <li>Vocabulary: 60000</li>
              <li>Number of layers: 3</li>
            </ul>
            <table>
              <thead>
                <tr>
                  <th>Machine</th>
                  <th>CPU/GPU</th>
                  <th>Size</th>
                  <th>Time</th>
                  <th>Perplexity</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><small>CSIE Workstation</small></td>
                  <td>E5 &#8776;500%</td>
                  <td>512</td>
                  <td>2 days</td>
                  <td>&#8776; 50</td>
                </tr>
                <tr>
                  <td><small>SpeechLab Workstation</small></td>
                  <td>GTX 960</td>
                  <td>128</td>
                  <td>6 hours</td>
                  <td>&#8776; 29</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h2>Demo Time</h2>
          </section>
        </section>

        <!-- END page -->
        <section>
            <h1>THE END</h1>
        </section>

      </div> <!-- END of div slides -->

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
          config: 'TeX-AMS_HTML-full'
        },

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/prism/prism.js', async: true },
          { src: 'plugin/math/math.js', async: true }
        ]
      });
    </script>

  </body>
</html>
