<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <title>Weekly Report V</title>
    <meta name="description" content="">
    <meta name="author" content="">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/white_edit.css" id="theme">
    <link rel="stylesheet" href="css/prism.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">

        <section>
          <h1>Weekly Report V</h1>
          <p>林義聖、陳力宇</p>
          <p>December 22, 2016</p>
        </section>

        <section>
          <section>
            <h2>Experiment</h2>
          </section>
          <section>
            <h2>Corpus</h2>
            <ul>
              <li>Ubuntu Dialog Data: <i>too many terminologies</i></li>
              <li>Cornell Movie-Dialogs: <i>more like common conversation</i></li>
            </ul>
            </section>
          <section>
            <h2>Cornell Movie-Dialogs</h2>
            <p>220,579 conversational exchanges <br> between 10,292 pairs of movie characters.</p>
            <ul>
              <li>involves 9,035 characters from 617 movies</li>
              <li>in total 304,713 utterances</li>
            </ul>
            <p><small><u>https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html</u></small></p>
          </section>
          <section>
            <h2>Preprocess</h2>
            <ul>
              <li>split by "\t"</li>
              <li>if a word appears twice or more, <br> add it to the vocabulary. (53346)</li>
            </ul>
          </section>
          <section>
            <h2>Demo Time</h2>
          </section>
          <section>
            <h2>Problems</h2>
            <ul>
              <li>commonplace responses</li>
              <li>unrelated to the question</li>
              <li>some words appeared repeatedly</li>
            </ul>
          </section>
          <section>
            <h2>Next step</h2>
            <p>use NLTK to preprocess</p>
            <ul>
              <li>plural → singular</li>
              <li>verb → infinitive</li>
              <li>number → _NUMBER</li>
              <li>name → _NAME</li>
            </ul>
          </section>
          <section>
            <h2>Next step</h2>
            <p>train MMI Model</p>
          </section>
        </section>

        <section>
          <section>
            <h2>A Diversity-Promoting<br>Objective Function<br>for Neural Conversation Models</h2>
            <p><small>
              Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan
              <br>
              Cite: arXiv:1510.03055 [cs.CL]
            </small></p>
          </section>
          <section>
            <h2>Introduction</h2>
            <p align="left"><i>SEQ2SEQ</i> models are able to learn semantic and syntactic relations between pairs, and capture contextual dependencies.</p>
            <p align="left">However, neural conversation models tend to generate <u>trivial</u> or <u>non-committal</u> responses, often involving high-frequency phrases.</p>
          </section>
          <section>
            <h2>Introduction</h2>
            <table>
              <caption><b>Input:</b> What are you doing?</caption>
              <thead>
                <tr>
                  <th>Prob.($\log$)</th>
                  <th>Response</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>-0.86</td>
                  <td>I don't know.</td>
                </tr>
                <tr>
                  <td>-1.06</td>
                  <td>Nothing.</td>
                </tr>
                <tr>
                  <td>-1.10</td>
                  <td>I'm talking to you.</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h2>Introduction</h2>
            <table>
              <caption><b>Input:</b> What is your name?</caption>
              <thead>
                <tr>
                  <th>Prob.($\log$)</th>
                  <th>Response</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>-0.91</td>
                  <td>I don't know.</td>
                </tr>
                <tr>
                  <td>-0.92</td>
                  <td>I don't know, sir.</td>
                </tr>
                <tr>
                  <td>-1.55</td>
                  <td>My name is Robert.</td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <h2>Introduction</h2>
            <p>By optimizing for the likelihood of outputs given inputs, neural models assign high probability to <u>safe</u> responses.</p>
          </section>
          <section>
            <h2>Maximum Mutual Information</h2>
            <ul>
              <li>The likelihood that a message will be provided to a given response</li>
              <li>Use MMI as an objective function to measures the mutual dependence between inputs and outputs</li>
            </ul>
          </section>
          <section>
            <h2>MMI Models</h2>
            <p align="left">
              The standard objective function for SEQ2SEQ models is the log-likelihood of target $T$ given source $S$:
            </p>
            \[\hat{T} = \underset{T}{arg\,max}\{\log p(T|S)\}\]
            <p align="left">
              In MMI, parameters are chosen to maximize mutual information between the source $S$ and the target $T$:
            </p>
            \[\log\frac{p(S,T)}{p(S)p(T)}\]
          </section>
          <section>
            <h2>MMI Models</h2>
            <p align="left">
              And this is what we want:
            </p>
            \[\hat{T} = \underset{T}{arg\,max}\{\log p(T|S)-\log p(T)\}\]
            <p align="left">
              We introduces a hyperparameter $\lambda$ that controls how much to penalize generic responses:
            </p>
            \[\hat{T} = \underset{T}{arg\,max}\{\log p(T|S)-\lambda\log p(T)\}\]
          </section>
          <section>
            <h2>MMI Models</h2>
            <p align="left">
              By knowing that,
            </p>
            \[\log p(T) = \log p(T|S) + \log p(S) - \log p(S|T)\]
            <p align="left">
              We can rewrite the function as,
            </p>
            \begin{align*}
              \hat{T} &= \underset{T}{arg\,max}\{\log p(T|S) - \lambda\log p(T)\}\\
                      &= \underset{T}{arg\,max}\{ (1-\lambda)\log p(T|S) + \lambda\log p(S|T) - \lambda\log p(S) \}\\
                      &= \underset{T}{arg\,max}\{ (1-\lambda)\log p(T|S) + \lambda\log p(S|T)\}
            \end{align*}
          </section>
          <section>
            <h2>MMI-antiLM (1/3)</h2>
            <p align="left">It penalizes not only high-frequency, generic responses, but also fluent ones and thus lead to ungrammatical outputs.</p>
            \[\log p(T|S)-\lambda\log p(T)\]
          </section>
          <section>
            <h2>MMI-antiLM (2/3)</h2>
            <p align="left"><b>Solution</b></p>
            \[ p(T) = \prod_{k=1}^{N_t} p(t_k|t_1,t_2,\dots,t_{k-1}) \]
            <p align="left">A weight $g(k)$ that is decremented monotonically,</p>
            \[ U(T) = \prod_{k=1}^{N_t} p(t_k|t_1,t_2,\dots,t_{k-1}) \cdot g(k) \]
          </section>
          <section>
            <h2>MMI-antiLM (3/3)</h2>
            <p align="left">Penalizing words predicted early on by the language model contributes more to the diversity of the sentence. We set up a threshold $\gamma$ by penalizing the first $\gamma$ words,</p>
            \begin{equation*}
              g(k) = \left\{
                \begin{aligned}
                & 1  & &\text{if } k \leq \gamma \\
                & 0  & &\text{if } k > \gamma
                \end{aligned}
              \right.
            \end{equation*}
            <p align="left">So now, rewrite the equation as:</p>
            \[ \log p(T|S) - \lambda\log U(T) \]
          </section>
          <section>
            <h2>MMI-bidi (1/3)</h2>
            <p align="left">Direct decoding from</p>
            \[ (1-\lambda)\log p(T|S)+\lambda\log p(S|T) \]
            <p align="left">is intractable, as the second part requires completion of target generation before $p(S|T)$ can be effectively computed.</p>
          </section>
          <section>
            <h2>MMI-bidi (2/3)</h2>
            \[ (1-\lambda)\log p(T|S)+\lambda\log p(S|T) \]
            <p align="left">For pratical reasons, we generate N-best lists given the first part of objective function, and then rerank the N-best lists using the second term of the objective function.</p>
          </section>
          <section>
            <h2>MMI-bidi (3/3)</h2>
            <p align="left">Drawbacks:</p>
            <ul>
              <li>Non-globally-optimal</li>
              <li>Rely heavily on the system's success in generating a sufficiently diverse N-best list</li>
              <li>Require a long N-best list be generated for each message</li>
            </ul>
            <p align="left">The results are grammatical and well-formed.</p>
          </section>
          <section>
            <h2>Experiments</h2>
            <table>
              <caption>Training datasets: Twitter conversation triple dataset (size: 23M)</caption>
              <thead>
                <tr>
                  <th>Model</th>
                  <th>BLEU</th>
                  <th>distinct-1</th>
                  <th>distince-2</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>SEQ2SEQ</td>
                  <td>4.31</td>
                  <td>.023</td>
                  <td>.107</td>
                </tr>
                <tr>
                  <td>MMI-antiLM</td>
                  <td>4.86</td>
                  <td>.033</td>
                  <td>.175</td>
                </tr>
                <tr>
                  <td>MMI-bidi</td>
                  <td>5.22</td>
                  <td>.051</td>
                  <td>.270</td>
                </tr>
              </tbody>
            </table>
          </section>
        </section>

        <!-- END page -->
        <section>
            <h1>THE END</h1>
        </section>

      </div> <!-- END of div slides -->

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        slideNumber: true,
        transition: 'slide', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
          config: 'TeX-AMS_HTML-full'
        },

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/prism/prism.js', async: true },
          { src: 'plugin/math/math.js', async: true }
        ]
      });
    </script>

  </body>
</html>
