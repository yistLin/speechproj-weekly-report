<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">
		<title>Weekly Report III</title>
		<meta name="description" content="">
		<meta name="author" content="">
		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white_edit.css" id="theme">
		<link rel="stylesheet" href="css/prism.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

    <style type="text/css">
      p.seq {
        font-size: 24px;
        margin-top:0;
        margin-bottom:0;
      }
      p.arrow {
        font-size: 30px;
        margin-top:0;
        margin-bottom:0;
      }
      p.vector {
        font-size: 24px;
        margin: 0 auto;
        border: 1px solid black;
        width: 80%;
      }
    </style>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<section>
					<h1>Weekly Report III</h1>
					<p>林義聖、陳力宇</p>
          <p>November 24, 2016</p>
				</section>

				<!-- Week 6 & 7 & 8 -->

        <section>
          <section>
            <h2>Sequence to Sequence Learning with Neural Networks</h2>
            <p>Ilya Sutskever, Oriol Vinyals, Quoc V.Le</p>
            <p><small><u>http://papers.nips.cc/paper/5346-information-based-learning-by-agents-in-unbounded-state-spaces.pdf</u></small></p>
          </section>
          <section>
            <h3>Introduction</h3>
            <ul>
              <li>It's an end-to-end approach to sequence learning</li>
              <li>
                Some features:
                <ol>
                  <li>Multilayered LSTM to map input sentence to a fixed-dimension vector</li>
                  <li>Another deep LSTM to decode vector to output sentence</li>
                  <li><i>Reverse</i> the order of the words in all source sentence</li>
                </ol>
              </li>
              <li>Machine translation on an English to French translation task from the <i>WMT'14</i> dataset</li>
            </ul>
          </section>
          <section data-transition="convex">
            <h3>Introduction</h3>
            <p class="seq">input sequence from source</p>
            <p class="arrow">&darr;</p>
            <p class="seq">(<i>reverse order</i>)</p>
            <p class="arrow">&darr;</p>
            <p class="seq">source from sentence input</p>
            <p class="arrow">&darr;</p>
            <p class="vector">LSTM ...</p>
            <p class="arrow">&darr;</p>
            <p class="vector">fixed-dim vector</p>
            <p class="arrow">&darr;</p>
            <p class="vector">LSTM ...</p>
            <p class="arrow">&darr;</p>
            <p class="seq">output sequence</p>
          </section>
          <section>
            <h3>Reversed Order</h3>
            <ul>
              <li>It's extremely valuable to reverse the order of the words</li>
              <li>Reverse the order of the words in the source sentence but not the target sentence</li>
              <li>Introduce many short term dependencies that made the optimization problem much simpler</li>
              <li><i>Adventage:</i> be able to do well on long sentences</li>
            </ul>
          </section>
          <section>
            <h3>Reversed Order</h3>
            <p align=left>
              When we try to translate <b>abc</b> to <b>&alpha;&beta;&gamma;</b>, the LSTM is asked to map:
            </p>
            <p>c, b, a &rarr; &alpha;, &beta;, &gamma;</p>
            <p align=left>This way, <b>a</b> is in close proximity to <b>&alpha;</b>, <b>b</b> is fairly close to <b>&beta;</b>, and so on.</p>
          </section>
          <section>
            <h3>Reversed Order</h3>
            <ul>
              <li>The average distance between corresponding words is unchanged</li>
              <li>The first few words are now very close in source and target sentences &rArr; Minimal time lag is greatly reduced</li>
              <li>Backpropagation has an easier time <i>establishing communication</i></li>
              <li>For long sentence, this method still works well. It results in LSTMs with better memory utilization</li>
            </ul>
          </section>
          <section>
            <h3>Fixed-dimensional Vector</h3>
            <ul>
              <li>Instead of directly mapping English to French, this model maps the source to a fixed-sized vector first</li>
              <li>
                The goal of the LSTM is to estimate the conditional probability
                \[p(y_1,\dots,y_T|x_1,\dots,x_T)\]
              </li>
              <li>
                When fixed-dimensional representation $v$ of the input sequence $(x_1,\dots,x_T)$ is given, the LSTM-LM can compute
                \[p(y_1,\dots,y_T|x_1,\dots,x_T)=\prod_{t=1}^{T'}p(y_t|v,y_1,\dots,y_{t-1})\]
              </li>
            </ul>
          </section>
          <section>
            <h3>Two LSTMs</h3>
            <ul>
              <li>People also use RNN to do this task. However, it's difficult to train the RNNs due to the resulting long term dependencies</li>
              <li>The encoding LSTM and decoding LSTM are different</li>
              <li>Deep LSTMs outperform shallow LSTMs(they used 4-layerd LSTM, each additional layer reduced perplexity by nearly 10%)</li>
            </ul>
          </section>
          <section>
            <h3>Result &amp; Comparision</h3>
            <ul>
              <li>BLEU score on WMT'14 dataset:
                <ol>
                  <li>Direct translations obtained a not bad score(with a vocabulary of only 80K words!), which is 34.81</li>
                  <li>The score obtained by an SMT baseline is 33.30</li>
                  <li>Use the LSTM to rescore the publicly available 1000-best lists of the SMT baseline get 36.5(the best result of the published result is 37.0)</li>
                </ol>
              </li>
              <li>This still has much room for improvement</li>
            </ul>
          </section>
        </section>

				<section>
					<section>
            <h2>Working on Tensorflow</h2>
					</section>
					<section>
						<h2>Language Modeling</h2>
						<p>to predict next words in a text given a history of previous words</p>
					</section>
					<section>
						<h2>Penn Tree Bank (PTB) dataset</h2>
						<ul>
							<li>Overall 10000 different words</li>
							<li>&ltunk&gt for rare words</li>
						</ul>
					</section>
					<section>
						<h2>Neural Network RNN Cells</h2>
						<p>RNN Cells for use with TensorFlow's core RNN methods</p>
						<ul>
							<li>BasciRNNCell</li>
							<li>BasicLSTMCell</li>
							<li>GRUCell</li>
							<li>LSTMCell</li>
						</ul>
					</section>
					<section>
						<h2>RNN Cell wrappers</h2>
						<p>RNNCells that wrap other RNNCells</p>
						<ul>
							<li>DropoutWrapper</li>
							<li>EmbeddingWrapper</li>
							<li>InputProjectionWrapper</li>
							<li>OutputProjectionWrapper</li>
						</ul>
					</section>
					<section>
						<h2>LSTM</h2>
					</section>
					<section>
						<h2>Truncated Backpropagation</h2>
					</section>
					<section>
						<h2>Stacking multiple LSTMs</h2>
					</section>
					<section>
						<h2>Decreasing learning rate schedule</h2>
					</section>
					<section>
						<h2>Dropout between the LSTM layers</h2>
					</section>
					<section>
						<h2>TensorBoard</h2>
						<p>Visualizing Learning</p>
					</section>
					<section>
						<h2>tf.Variables</h2>
						<ul>
							<li>Creation</li>
							<li>Initialization</li>
							<li>Saving</li>
							<li>Loading</li>
						</ul>
					</section>
				</section>

        <!-- END page -->
        <section>
            <h1>THE END</h1>
        </section>

      </div> <!-- END of div slides -->

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>
		<script>
			// More info https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,
				transition: 'slide', // none/fade/slide/convex/concave/zoom
        math: {
          mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
          config: 'TeX-AMS_HTML-full'
        },

				// More info https://github.com/hakimel/reveal.js#dependencies
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/prism/prism.js', async: true },
          { src: 'plugin/math/math.js', async: true }
				]
			});
		</script>

	</body>
</html>
